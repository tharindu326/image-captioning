{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 06\n",
    "# Loading Models\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = nn.Sequential(*list(torchvision.models.resnet50(pretrained=True).children())[:-1])\n",
    "        self.linear = nn.Linear(2048, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        out_linear = self.linear(features)\n",
    "        embeddings = self.bn(out_linear)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(SemanticEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        embeddings = self.embedding(texts)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "\n",
    "        return hiddens[:, -1]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        # lstm cell\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embed_size + hidden_size, hidden_size=hidden_size)\n",
    "        # output fully connected layer\n",
    "        self.fc_out = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size + hidden_size, embedding_dim=embed_size + hidden_size)\n",
    "        # activations\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # batch size\n",
    "        batch_size = features.size(0)\n",
    "        # init the hidden and cell states to zeros\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        # define the output tensor placeholder\n",
    "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n",
    "        # embed the captions\n",
    "        captions_embed = self.embed(captions)\n",
    "        # pass the caption word by word\n",
    "        for t in range(captions.size(1)):\n",
    "            # for the first time step the input is the feature vector\n",
    "            if t == 0:\n",
    "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "            # for the 2nd+ time step, using teacher forcer\n",
    "            else:\n",
    "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
    "            # output of the attention mechanism\n",
    "            out = self.fc_out(hidden_state)\n",
    "            # build the output tensor\n",
    "            outputs[:, t, :] = out\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, max_seg_length):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        # inputs = features.unsqueeze(1)\n",
    "        batch_size = features.size(0)\n",
    "        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
    "        for i in range(max_seg_length):\n",
    "            # hiddens, states = self.lstm(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
    "            hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
    "            # outputs = self.linear(hiddens.squeeze(1))  # outputs:  (batch_size, vocab_size)\n",
    "            outputs = self.fc_out(hidden_state)\n",
    "            _, predicted = outputs.max(1)  # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            features = self.embed(predicted)  # inputs: (batch_size, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)  # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iNFERENCE PIPELINE FOR GENERATE CAPTIONS\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pickle\n",
    "from ipynb.fs.full.vocabulary_builder import Vocabulary\n",
    "from ipynb.fs.full.data_loader import ROCODataset\n",
    "\n",
    "\n",
    "class CaptionGenerator:\n",
    "    def __init__(self, vocab_file, embed_size, hidden_size, num_layers, image_encoder_path, semantic_encoder_path,\n",
    "                 decoder_path):\n",
    "        \n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "        # convert vocab into index2word format\n",
    "        vocab_size = 2200\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            map_location = torch.device('cuda')\n",
    "        else:\n",
    "            map_location = torch.device('cpu')\n",
    "\n",
    "        self.image_encoder = ImageEncoder(embed_size).to(self.device)\n",
    "        self.semantic_encoder = SemanticEncoder(vocab_size, embed_size, hidden_size, num_layers).to(self.device)\n",
    "        self.decoder = Decoder(vocab_size, embed_size, hidden_size).to(self.device)\n",
    "\n",
    "        # Load trained weights (assuming you saved them earlier as 'encoder.pth' and 'decoder.pth')\n",
    "        self.image_encoder.load_state_dict(torch.load(image_encoder_path, map_location=map_location))\n",
    "        self.semantic_encoder.load_state_dict(torch.load(semantic_encoder_path, map_location=map_location))\n",
    "        self.decoder.load_state_dict(torch.load(decoder_path, map_location=map_location))\n",
    "\n",
    "        # Set to evaluation mode\n",
    "        self.image_encoder.eval()\n",
    "        self.semantic_encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def generate_caption(self, image_path, max_length=20):\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Get the feature vector from the encoder\n",
    "        features_image = self.image_encoder(image)\n",
    "        # tokens = word_tokenize('CTLA venous phase of donor'.lower())\n",
    "        caption = [self.vocab('<START>')]\n",
    "        # caption.extend([self.vocab.get(token, self.vocab['<UNK>']) for token in tokens])\n",
    "        # caption.append(self.vocab['<END>'])\n",
    "        features_text = self.semantic_encoder(torch.Tensor([caption]).long()[:, :torch.Tensor([caption]).shape[1]])\n",
    "\n",
    "        combined_features = torch.cat((features_image, features_text), dim=1)\n",
    "        gen_caption = []\n",
    "        predicted_token_ids = self.decoder.sample(combined_features, max_seg_length=20)\n",
    "        for id in predicted_token_ids[0].tolist():\n",
    "            word = self.vocab_i2w[id]\n",
    "            if word == '<END>':\n",
    "                break\n",
    "            gen_caption.append(word)\n",
    "        return ' '.join(gen_caption)\n",
    "\n",
    "    def process_batch(self, batch_size, max_seg_length, json_data_path='selected_dataset/selected_dataset_info.json'):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        test_dataset = ROCODataset(data_json='selected_dataset/selected_dataset_info.json',\n",
    "                                transform=transform,\n",
    "                                vocab=self.vocab,\n",
    "                                dataset_type='test')\n",
    "\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                collate_fn=ROCODataset.collate_fn,\n",
    "                                num_workers=2)\n",
    "\n",
    "        processed_captions = []\n",
    "        GT_captions = []\n",
    "        for i, (images_val, GT, _) in enumerate(test_loader):\n",
    "            caps = []\n",
    "            for i in range(images_val.size()[0]):\n",
    "                caps.append([self.vocab('<START>')])\n",
    "            captions_val = torch.Tensor(caps).long()\n",
    "            \n",
    "            images = images_val.to(self.device)\n",
    "            image_features = self.image_encoder(images)\n",
    "            text_features = self.semantic_encoder(captions_val[:, :captions_val.shape[1]].to(self.device))\n",
    "            combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "            \n",
    "            # captions_target = captions_val.to(self.device)\n",
    "            # predictions = torch.argmax(self.decoder(combined_features, captions_target), dim=2)\n",
    "            \n",
    "            predictions = self.decoder.sample(combined_features, max_seg_length=max_seg_length)\n",
    "            \n",
    "            for prediction in predictions:\n",
    "                # Convert word_ids to words\n",
    "                sampled_caption = []\n",
    "                for word_id in prediction.detach().numpy():\n",
    "                    word = self.vocab.idx2word[word_id]\n",
    "                    if word != '<start>':\n",
    "                        if word != '<unk>':\n",
    "                            if word != '<end>':\n",
    "                                sampled_caption.append(word)\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                image_caption = ' '.join(sampled_caption)\n",
    "                processed_captions.append(image_caption)\n",
    "            \n",
    "            # getting GTs\n",
    "            for gt in GT:\n",
    "                ground_truth_caption = []\n",
    "                for word_id in gt.detach().numpy():\n",
    "                    word = self.vocab.idx2word[word_id]\n",
    "                    if word != '<start>':\n",
    "                        if word != '<unk>':\n",
    "                            if word != '<end>':\n",
    "                                if word != '<pad>':\n",
    "                                    ground_truth_caption.append(word)\n",
    "                image_gt_caption = ' '.join(ground_truth_caption)\n",
    "                GT_captions.append(image_gt_caption)\n",
    "        return processed_captions, GT_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoomi\\Desktop\\Tharindu\\oulu - masters\\NLP\\project\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from itertools import combinations\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "\n",
    "# Load pre-trained Word2Vec and FastText models\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('crawl-300d-2M.vec')\n",
    "\n",
    "\n",
    "def get_embedding(sentence, embed_type):\n",
    "    if embed_type == \"biobert\":\n",
    "        # Load pre-trained model and tokenizer for BioBERT\n",
    "        model_name = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "        biobert = BertModel.from_pretrained(model_name)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            outputs = biobert(**inputs)\n",
    "            return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    else:\n",
    "        words = word_tokenize(sentence)\n",
    "        if embed_type == \"word2vec\":\n",
    "            vectors = [word2vec_model[word] for word in words if word in word2vec_model.key_to_index]\n",
    "        elif embed_type == \"fasttext\":\n",
    "            vectors = [fasttext_model[word] for word in words if word in fasttext_model.key_to_index]\n",
    "        else:\n",
    "            raise ValueError(f\"unknown {embed_type}\")\n",
    "        return np.mean(vectors, axis=0).reshape(1, -1)\n",
    "\n",
    "\n",
    "def calculate_similarities(original, prediction, embedding_type):\n",
    "    orig_embedding = get_embedding(original, embedding_type)\n",
    "    pred_embedding = get_embedding(prediction, embedding_type)\n",
    "\n",
    "    # Compute similarities\n",
    "    cosine_sim = cosine_similarity(orig_embedding, pred_embedding)[0][0]\n",
    "    euclidean_dist = distance.euclidean(orig_embedding.flatten(), pred_embedding.flatten())\n",
    "    manhattan_dist = distance.cityblock(orig_embedding.flatten(), pred_embedding.flatten())\n",
    "    bleu_score = sentence_bleu([word_tokenize(original)], word_tokenize(prediction), weights=(0, 1, 0, 0))  # bi-gram\n",
    "    jaccard_sim = jaccard_similarity(word_tokenize(original), word_tokenize(prediction))\n",
    "\n",
    "    return {\n",
    "        \"Cosine Similarity\": cosine_sim,\n",
    "        \"Euclidean Distance\": euclidean_dist,\n",
    "        \"Manhattan Distance\": manhattan_dist,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"Jaccard Similarity\": jaccard_sim\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_countvector_similarity(original, prediction):\n",
    "    # Compute similarities based on count vectors\n",
    "    vectorizer = CountVectorizer().fit([original, prediction])\n",
    "\n",
    "    # Convert sentences to count vectors\n",
    "    orig_vector = vectorizer.transform([original]).toarray()[0]  # Convert sparse matrix to numpy array\n",
    "    pred_vector = vectorizer.transform([prediction]).toarray()[0]  # Convert sparse matrix to numpy array\n",
    "\n",
    "    # Compute similarities based on count vectors\n",
    "    cosine_sim = cosine_similarity([orig_vector], [pred_vector])[0][0]\n",
    "    euclidean_dist = distance.euclidean(orig_vector, pred_vector)\n",
    "    manhattan_dist = distance.cityblock(orig_vector, pred_vector)\n",
    "    bleu_score = sentence_bleu([word_tokenize(original)], word_tokenize(prediction), weights=(0, 1, 0, 0))  # bi-gram\n",
    "    jaccard_sim = jaccard_similarity(word_tokenize(original), word_tokenize(prediction))\n",
    "\n",
    "    return {\n",
    "        \"Cosine Similarity\": cosine_sim,\n",
    "        \"Euclidean Distance\": euclidean_dist,\n",
    "        \"Manhattan Distance\": manhattan_dist,\n",
    "        \"BLEU Score\": bleu_score,\n",
    "        \"Jaccard Similarity\": jaccard_sim\n",
    "    }\n",
    "\n",
    "\n",
    "def sent2sent_similarity(captionA, captionB):\n",
    "    \"\"\"\n",
    "    Question6: get the similarity between GT and the generated caption\n",
    "    :param captionA: input caption/ Generated caption (Y)\n",
    "    :param captionB: GT (X)\n",
    "    :return: json object with different embeddings types and their similarity scores\n",
    "    EX: {\n",
    "            \"embed_type\": {\n",
    "                \"Cosine Similarity\": float,\n",
    "                \"Euclidean Distance\": float,\n",
    "                \"Manhattan Distance\": float,\n",
    "                \"BLEU Score\": float,\n",
    "                \"Jaccard Similarity\": float\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for method in [\"biobert\", \"word2vec\", \"fasttext\"]:\n",
    "        results[method] = calculate_similarities(captionA, captionB, method)\n",
    "    results['countvectorizer'] = calculate_countvector_similarity(captionA, captionB)\n",
    "    return results\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zoomi\\Desktop\\Tharindu\\oulu - masters\\NLP\\project\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Zoomi\\Desktop\\Tharindu\\oulu - masters\\NLP\\project\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated_caption within within within within within within within within within\n",
      "GT_caption coronal ct scan taken two years shows enlarged kidneys filled with numerous cysts and occupying the abdominal and pelvic . note that there are only a few cysts in the liver .\n",
      "{'biobert': {'Cosine Similarity': 0.737587, 'Euclidean Distance': 9.570383071899414, 'Manhattan Distance': 209.31915, 'BLEU Score': 0, 'Jaccard Similarity': 0.0}, 'word2vec': {'Cosine Similarity': 0.2919802, 'Euclidean Distance': 2.1420795917510986, 'Manhattan Distance': 30.315865, 'BLEU Score': 0, 'Jaccard Similarity': 0.0}, 'fasttext': {'Cosine Similarity': 0.40645656, 'Euclidean Distance': 3.2084531784057617, 'Manhattan Distance': 41.06874, 'BLEU Score': 0, 'Jaccard Similarity': 0.0}, 'countvectorizer': {'Cosine Similarity': 0.0, 'Euclidean Distance': 10.770329614269007, 'Manhattan Distance': 38, 'BLEU Score': 0, 'Jaccard Similarity': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "path_image_encoder = 'train/image_encoder.pth'\n",
    "path_semantic_encoder_path = 'train/semantic_encoder.pth'\n",
    "path_decoder_path = 'train/decoder.pth'\n",
    "# Initialize the caption generator and generate caption\n",
    "caption_generator = CaptionGenerator(vocab_file='vocab.pkl', embed_size=256, hidden_size=256, num_layers=1,\n",
    "                                        image_encoder_path=path_image_encoder,\n",
    "                                        semantic_encoder_path=path_semantic_encoder_path,\n",
    "                                        decoder_path=path_decoder_path)\n",
    "# image_path = \"selected_dataset/test/radiology/images/PMC4803869_GJHS-7-124-g006.jpg\"\n",
    "# caption = caption_generator.generate_caption(image_path)\n",
    "generated_captions, GTs = caption_generator.process_batch(batch_size=4, max_seg_length=10)\n",
    "\n",
    "# use first image \n",
    "Generated_caption = generated_captions[0]\n",
    "GT_caption = GTs[0]\n",
    "print('Generated_caption', Generated_caption)\n",
    "print('GT_caption', GT_caption)\n",
    "\n",
    "results = sent2sent_similarity(Generated_caption, GT_caption)\n",
    "print(results)\n",
    "# print(json.dumps(results, default=lambda o: float(o) if isinstance(o, np.float32) else o, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}, {'similarity_score': 0.3779644730092272, 'best_matching_caption': ' Gallstone remains impacted within the sigmoid colon.'}, {'similarity_score': 0.19611613513818404, 'best_matching_caption': ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire'}]\n"
     ]
    }
   ],
   "source": [
    "# Question 07: get the most similar caption for generated caption from training set\n",
    "def get_best_match_from_target_list(caption, json_data):\n",
    "    \"\"\"\n",
    "    :param caption: input caption/ generated caption (Y)\n",
    "    :param json_data: data file to get the training list\n",
    "    :return: best matching caption for Y <-- (Z)\n",
    "    \"\"\"\n",
    "    data = json.load(open(json_data))\n",
    "    target_captions = []\n",
    "    for k, v in data.items():\n",
    "        if k == 'train':\n",
    "            for item in v:\n",
    "                target_captions.append(list(item.values())[0]['caption'])\n",
    "\n",
    "    best_sim = 0\n",
    "    best_caption = None\n",
    "    for train_cap in target_captions:\n",
    "        cosine_sim = calculate_countvector_similarity(train_cap, caption)\n",
    "        if best_sim < cosine_sim['Cosine Similarity']:\n",
    "            best_sim = cosine_sim['Cosine Similarity']\n",
    "            best_caption = train_cap\n",
    "    return best_sim, best_caption\n",
    "\n",
    "#Q7\n",
    "# get the most similar caption for generated caption from training set, for all the test images \n",
    "json_data = 'selected_dataset/selected_dataset_info.json'\n",
    "best_similarities_Z = []\n",
    "best_matching_captions_Z = []\n",
    "all_data = []\n",
    "for i, Generated_caption in enumerate(generated_captions):\n",
    "    best_similarity, best_matching_caption = get_best_match_from_target_list(Generated_caption, json_data)\n",
    "    best_similarities_Z.append(best_similarity)\n",
    "    best_matching_captions_Z.append(best_matching_caption)\n",
    "    all_data.append({'similarity_score': best_similarity, 'best_matching_caption': best_matching_caption})\n",
    "print(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire', ' Gallstone remains impacted within the sigmoid colon.', ' TDM abdomino pelvienne en coupe axiale montrant une masse tissulaire retro péritonéale gauche faisant 8x5 cm à contours bosselé en rapport avec un amas ganglionnaire']\n"
     ]
    }
   ],
   "source": [
    "# Question 8: compare GT (X) with (Y) and (Z) and get the best caption as (Y) or (Z) and assigned it to test image\n",
    "def get_best_caption(X_Y_sim_score, X_Z_sim_score, Y, Z):\n",
    "    \"\"\"\n",
    "    :param Z: retrival caption from training set for generated caption\n",
    "    :param Y: generated caption\n",
    "    :param X_Y_sim_score: similarity score between GT (X) and generated caption (Y)\n",
    "    :param X_Z_sim_score: similarity score between GT (X) and retrival caption from training set (Z)\n",
    "    :return: (Y) or (Z)\n",
    "    \"\"\"\n",
    "    if X_Y_sim_score > X_Z_sim_score:\n",
    "        ret = Y\n",
    "    else:\n",
    "        ret = Z\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Q8\n",
    "final_captions = []\n",
    "for i, (GT_caption, Generated_caption) in enumerate(zip(GTs, generated_captions)):\n",
    "    cosine_sim = calculate_countvector_similarity(GT_caption, Generated_caption)\n",
    "    final_caption = get_best_caption(cosine_sim['Cosine Similarity'], best_similarities_Z[i], Generated_caption, best_matching_captions_Z[i])\n",
    "    final_captions.append(final_caption)\n",
    "print(final_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score: 0.6073\n"
     ]
    }
   ],
   "source": [
    "# Question9: average similarity value for the captions from all test images\n",
    "def compute_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score between a reference and a candidate sentence.\n",
    "    \"\"\"\n",
    "    # BLEU expects tokenized sentences\n",
    "    reference = reference.split()\n",
    "    candidate = candidate.split()\n",
    "\n",
    "    # Use a smoothing function to avoid issues with zero BLEU scores\n",
    "    smoothing = SmoothingFunction().method1\n",
    "\n",
    "    return sentence_bleu([reference], candidate, smoothing_function=smoothing)\n",
    "\n",
    "\n",
    "def get_average_similarity(captions):\n",
    "    \"\"\"\n",
    "    Q9:\n",
    "    :param captions: list of final captions for the test images\n",
    "    :return: average similarity value for the captions from all test images\n",
    "    Using BLEU score similarity\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # Compute pairwise BLEU scores for a list of sentences.\n",
    "    for s1, s2 in combinations(captions, 2):\n",
    "        bleu_score = compute_bleu(s1, s2)\n",
    "        scores.append(bleu_score)\n",
    "        # print(f\"BLEU({s1}, {s2}) = {bleu_score:.4f}\")\n",
    "\n",
    "    # get average\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    return avg_score\n",
    "\n",
    "# Q9\n",
    "avg_sim = get_average_similarity(generated_captions)\n",
    "print(f\"Average BLEU score: {avg_sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
